import json

import numpy
from transformers import AutoTokenizer
import torch
import random
import numpy as np
import sklearn.metrics as metrics
from torch.utils.data import DataLoader
from torch.nn.utils.rnn import pad_sequence
# from utils import Twitter_Set, Weibo_Set, Sarcasm_Set
import math


class PadCollate:
    def __init__(self, img_dim=0, text_dim=1, text_dep_dim=2, label_dim=3, caption_dim=4,
                 max_cap_len=20, ch=False, graph_type='cross'):
        """
        Args:
            img_dim (int): dimension for the image bounding boxes
            embed_dim1 (int): dimension for the matching caption
            embed_dim2 (int): dimension for the non-matching caption
            type
        """
        self.img_dim = img_dim
        self.text_dim = text_dim
        self.text_dep_dim = text_dep_dim
        self.label_dim = label_dim

        self.caption_dim = caption_dim
        self.max_cap_len = max_cap_len
        # img, twitter, dep, label, knowledge
        self.ch = ch
        self.graph_type = graph_type
        # self.img_patch = img_patch
        if self.ch:
            self.tokenizer = AutoTokenizer.from_pretrained('bert/bert-base-chinese')
        else:
            self.tokenizer = AutoTokenizer.from_pretrained('bert/bert-base-uncased')

        # self.mask_V = torch.zeros()

    def pad_collate(self, batch):
        imgs = list(map(lambda t: t[self.img_dim].clone().detach(), batch))
        # [N, 49, 768])
        imgs = torch.stack(imgs)
        # text
        texts = list(map(lambda t: t[self.text_dim], batch))
        token_lens = [len(twitter) for twitter in texts]

        encoded_texts = self.tokenizer(texts, is_split_into_words=True, return_tensors="pt", truncation=True,
                                       max_length=150, padding=True)
        # image caption
        img_patch_lens = [len(img) for img in imgs]
        # construct the map between tokens generated by bert and ones generated by spacy for image caption
        # assure that the maximum length is less ore equal to max_cap_len
        # construct the map between tokens generated by bert and ones generated by spacy for text
        word_spans = []
        word_len = []
        for index_encode, len_token in enumerate(token_lens):
            word_span_ = []
            for i in range(len_token):
                word_span = encoded_texts[index_encode].word_to_tokens(i)
                if word_span is not None:
                    # delete [CLS]
                    word_span_.append([word_span[0] - 1, word_span[1] - 1])
            word_spans.append(word_span_)
            # final length of text
            word_len.append(len(word_span_))
        max_len1 = max(word_len)
        # construct mask for  cross-attention between image and image caption
        mask_batch_text = construct_mask_text(word_len, max_len1)
        # construct cross-modal graph based on text dependency
        deps_caption = [x[self.text_dep_dim] for x in batch]
        deps_text_ = []
        for dep in deps_caption:
            deps_text_.append([d for d in dep if d[0] < max_len1 and d[1] < max_len1])

        labels = torch.tensor(list(map(lambda t: t[self.label_dim], batch)), dtype=torch.long)
        if self.graph_type == 'cross':
            adj_matrix = construct_adj_cross_graph(text_len=word_len, deps=deps_text_, img_len=img_patch_lens)
        # elif self.graph_type == 'uni':
        #     adj_matrix = construct_uni_modal_graph(text_len=word_len, deps=deps_text_, max_length=max_len1,
        #                               img_len=img_patch_lens)
        elif self.graph_type == 'hetero':
            pass
        else:
            adj_matrix = None
            print("Wrong graph type parameter")
            exit()
        mask_T_T = construct_mask_TT(mask=mask_batch_text, seq_len=word_len)
        mask_T_V = construct_mask_TV(mask=mask_batch_text, seq_len=word_len, img_patch=49)
        return imgs, encoded_texts,  mask_batch_text, adj_matrix, word_len, mask_T_T, mask_T_V, word_spans, labels

    def __call__(self, batch):
        return self.pad_collate(batch)


def construct_mask_text(seq_len, max_length):
    """

    Args:
        seq_len1(N): list of number of words in a caption without padding in a minibatch
        max_length: the dimension one of shape of embedding of captions of a batch

    Returns:
        mask(N,max_length): Boolean Tensor
    """
    # the realistic max length of sequence
    mask = torch.stack(
        [torch.cat([torch.zeros(len, dtype=bool), torch.ones(max_length - len, dtype=bool)]) for len in seq_len])
    return mask


def construct_mask_TT(mask, seq_len):
    """

    Args:
        seq_len1(N): list of number of words in a caption without padding in a minibatch
        max_length: the dimension one of shape of embedding of captions of a batch

    Returns:
        mask(N,max_length): Boolean Tensor
    """
    # the realistic max length of sequence N, L*L
    mask_TT = []
    max_length = mask.size(1)
    for i in range(len(seq_len)):
        mask_ = torch.cat([mask[i].unsqueeze(0).expand(seq_len[i], -1),
                           torch.ones((max_length - seq_len[i], max_length), dtype=bool)], dim=0)
        mask_TT.append(mask_)
    mask_TT = torch.stack(mask_TT, dim=0)
    return mask_TT


def construct_mask_TV(mask, seq_len, img_patch=49):
    """

    Args:
        seq_len1(N): list of number of words in a caption without padding in a minibatch
        max_length: the dimension one of shape of embedding of captions of a batch

    Returns:
        mask(N,max_length): Boolean Tensor
    """
    #
    mask_TV = []
    max_length = mask.size(1)
    for i in range(len(seq_len)):
        mask_ = torch.cat([torch.zeros((seq_len[i], img_patch), dtype=bool),
                           torch.ones((max_length - seq_len[i], img_patch), dtype=bool)], dim=0)
        mask_TV.append(mask_)
    mask_TV = torch.stack(mask_TV, dim=0)
    return mask_TV


def construct_adj_cross_graph(text_len, deps, img_len):
    max_text_length = max(text_len)
    max_img_length = max(img_len)
    p = math.sqrt(max_img_length)
    edge_image = torch.eye(n=max_img_length, m=max_img_length)
    for i in range(max_img_length):
        for j in range(max_img_length):
            # if j == i:
            #     continue
            if math.fabs(i % p - j % p) <= 1 and math.fabs(i // p - j // p) <= 1:
                edge_image[i, j] = 1
    adj_matrix = []
    for i in range(0, len(text_len)):
        tmp = torch.eye(n=(max_text_length + max_img_length), m=(max_text_length + max_img_length))
        # initialize text graph based on textual dep among tokens
        for d in deps[i]:
            tmp[d[0], d[1]] = 1
            tmp[d[1], d[0]] = 1
        # cross modal graph
        adj_matrix.append(tmp)
    adj_matrix = torch.stack(adj_matrix, dim=0)
    adj_matrix[:, 0:max_text_length, max_text_length:] = 1
    adj_matrix[:, max_text_length:, 0:max_text_length] = 1
    # visual graph
    adj_matrix[:, max_text_length:, max_text_length:] = edge_image
    adj_matrix = adj_matrix.detach()
    return adj_matrix


def construct_adj_cross_graph_no_dep(text_len, deps, max_length, img_len):
    adj_matrix = None
    return adj_matrix


def construct_uni_modal_graph(text_len, deps, max_length, img_len):
    adj_matrix = None
    return adj_matrix


def seed_everything(seed: int = 0):
    # seed setting
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False


def all_metrics(labels, predicted_labels):
    """
    :param real: list. length of list is N.
    :param predict:  list. length of list is N.
    :return all_metric: [acc, r_sarcasm, p_sarcasm, f1_sarcasm, r_non_sarcasm, p_non_sarcasm, f1_non_sarcasm]
    """
    confusion = metrics.confusion_matrix(labels, predicted_labels)
    total = confusion[0][0] + confusion[0][1] + confusion[1][0] + confusion[1][1]
    acc = (confusion[0][0] + confusion[1][1]) / total
    # about sarcasm
    r_sarcasm = confusion[1][1] / (confusion[1][1] + confusion[1][0])
    p_sarcasm = confusion[1][1] / (confusion[1][1] + confusion[0][1])
    print(confusion[0][0], confusion[0][1], confusion[1][0], confusion[1][1])
    f1_sarcasm = 2 * r_sarcasm * p_sarcasm / (r_sarcasm + p_sarcasm)
    # about non_sarcasm
    r_non_sarcasm = confusion[0][0] / (confusion[0][0] + confusion[0][1])
    if confusion[0][0] + confusion[1][0] == 0:
        p_non_sarcasm = 0
    else:
        p_non_sarcasm = confusion[0][0] / (confusion[0][0] + confusion[1][0])
    f1_non_sarcasm = 2 * r_non_sarcasm * p_non_sarcasm / (r_non_sarcasm + p_non_sarcasm)
    all_metric = [acc, r_sarcasm, p_sarcasm, f1_sarcasm, r_non_sarcasm, p_non_sarcasm, f1_non_sarcasm]
    for i in range(len(all_metric)):
        if np.isnan(all_metric[i]):
            all_metric[i] = 0
    return all_metric
